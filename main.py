import requests
import re
import json
import os
from collections import defaultdict
from datetime import datetime
import dns.resolver
from config import (
    UPSTREAM_RULES, OUTPUT_FILE, SUPPORTED_RULE_TYPES, EXCLUDED_PREFIXES,
    DNS_SERVERS, RESOLVED_CACHE_FILE, MYLIST_FILE
)

def download_rule(url: str) -> list[str]:
    """ä¸‹è½½å•ä¸ªä¸Šæ¸¸è§„åˆ™ï¼Œè¿”å›æœ‰æ•ˆè§„åˆ™åˆ—è¡¨ï¼ˆè¿‡æ»¤æ³¨é‡Š/ç©ºè¡Œï¼‰"""
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        }
        response = requests.get(url, headers=headers, timeout=60)
        response.raise_for_status()
        response.encoding = response.apparent_encoding or "utf-8"
        rules = response.text.split("\n")
        comment_prefixes = EXCLUDED_PREFIXES[:3]
        valid_rules = [
            rule.strip() for rule in rules
            if rule.strip() and not rule.strip().startswith(comment_prefixes)
        ]
        print(f"âœ… æˆåŠŸä¸‹è½½ {url} | æœ‰æ•ˆè§„åˆ™æ•°ï¼š{len(valid_rules)}")
        return valid_rules
    except requests.exceptions.ConnectionError:
        print(f"âŒ ä¸‹è½½å¤±è´¥ {url} | é”™è¯¯ï¼šç½‘ç»œè¿æ¥è¶…æ—¶/æ— æ³•è®¿é—®")
        return []
    except requests.exceptions.HTTPError as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥ {url} | é”™è¯¯ï¼šHTTP çŠ¶æ€ç  {e.response.status_code}")
        return []
    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥ {url} | é”™è¯¯ï¼š{str(e)}")
        return []

def convert_hosts_to_adguard(rule: str) -> str | None:
    """å°† Hosts è§„åˆ™è½¬æ¢ä¸º AdGuard è§„åˆ™ ||åŸŸå^"""
    hosts_pattern = r"^(0\.0\.0\.0|127\.0\.0\.1|::1)\s+([a-zA-Z0-9.-]+\.[a-zA-Z]+)"
    match = re.match(hosts_pattern, rule)
    if match:
        domain = match.group(2)
        return f"||{domain}^"
    return None

def parse_domain(rule: str) -> str | None:
    """ä»è§„åˆ™ä¸­æå–çº¯åŸŸåï¼ˆå”¯ä¸€æ ‡è¯†ï¼‰"""
    # åŒ¹é… ||domain^... æˆ– @@||domain^... æ ¼å¼ï¼Œæå–domain
    pattern = r"^(?:@@)?\|\|([a-zA-Z0-9.-]+\.[a-zA-Z]+)(\^.*)?"
    match = re.match(pattern, rule)
    if match:
        return match.group(1).lower()  # ç»Ÿä¸€å°å†™ï¼Œç¡®ä¿å”¯ä¸€æ€§
    return None

def check_domain_resolvable(domain: str) -> bool:
    """æ£€æŸ¥åŸŸåæ˜¯å¦å¯è¢«æŒ‡å®šDNSæœåŠ¡å™¨è§£æï¼ˆè‡³å°‘ä¸€ä¸ªæœ‰Aè®°å½•ï¼‰"""
    resolver = dns.resolver.Resolver()
    resolver.timeout = 5
    resolver.lifetime = 5
    for server in DNS_SERVERS:
        try:
            resolver.nameservers = [server]
            resolver.query(domain, 'A')
            return True
        except (dns.resolver.NXDOMAIN, dns.resolver.NoAnswer, dns.resolver.Timeout):
            continue
    return False

def load_resolved_cache() -> dict[str, bool]:
    """åŠ è½½å·²è§£æåŸŸåçš„ç¼“å­˜"""
    if os.path.exists(RESOLVED_CACHE_FILE):
        try:
            with open(RESOLVED_CACHE_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except json.JSONDecodeError:
            return {}
    return {}

def save_resolved_cache(cache: dict[str, bool]):
    """ä¿å­˜å·²è§£æåŸŸåçš„ç¼“å­˜"""
    with open(RESOLVED_CACHE_FILE, "w", encoding="utf-8") as f:
        json.dump(cache, f, ensure_ascii=False, indent=2)

def load_mylist() -> dict[str, str]:
    """åŠ è½½äººå·¥å®¡æŸ¥è§„åˆ™æ–‡ä»¶ï¼Œè¿”å›{çº¯åŸŸå: è§„åˆ™}"""
    mylist = {}
    if os.path.exists(MYLIST_FILE):
        with open(MYLIST_FILE, "r", encoding="utf-8") as f:
            for line in f:
                rule = line.strip()
                if not rule or rule.startswith(EXCLUDED_PREFIXES[:3]):  # è·³è¿‡æ³¨é‡Šå’Œç©ºè¡Œ
                    continue
                domain = parse_domain(rule)
                if domain:
                    mylist[domain] = rule
    return mylist

def merge_rules(all_rules: list[str]) -> tuple[list[str], list[str], list[str]]:
    """
    æ•´åˆè§„åˆ™ï¼šå¤„ç†mylistå†²çªã€DNSéªŒè¯ã€é»‘ç™½åå•å†²çª
    è¿”å›ï¼š(mylistå†²çªè§„åˆ™, é»‘ç™½åå•å†²çªè§„åˆ™, æ™®é€šè§„åˆ™)
    """
    # åŠ è½½mylistå’Œè§£æç¼“å­˜
    mylist = load_mylist()
    resolved_cache = load_resolved_cache()
    new_resolved = {}  # æœ¬æ¬¡è¿è¡Œæ–°å¢çš„è§£æç»“æœ

    # åˆ†ç»„ï¼šæŒ‰çº¯åŸŸå
    domain_groups = defaultdict(list)  # {çº¯åŸŸå: [è§„åˆ™åˆ—è¡¨]}

    # å¤„ç†ä¸Šæ¸¸è§„åˆ™ï¼Œæå–çº¯åŸŸåå¹¶è¿‡æ»¤æ— æ•ˆè§„åˆ™
    for rule in all_rules:
        # è½¬æ¢Hostsè§„åˆ™
        converted_rule = convert_hosts_to_adguard(rule)
        final_rule = converted_rule if converted_rule else rule

        # æå–çº¯åŸŸå
        domain = parse_domain(final_rule)
        if not domain:
            continue  # æ— æ³•æå–åŸŸåçš„è§„åˆ™è·³è¿‡

        domain_groups[domain].append(final_rule)

    # å¤„ç†å„ç»„è§„åˆ™
    mylist_conflict_rules = []  # mylistå†²çªçš„è§„åˆ™ï¼ˆä½¿ç”¨mylistçš„ï¼‰
    black_white_conflict_rules = []  # é»‘ç™½åå•å†²çªçš„è§„åˆ™ï¼ˆä¿ç•™ç™½åå•ï¼‰
    normal_rules = []  # æ™®é€šè§„åˆ™
    processed_domains = set()

    # å…ˆå¤„ç†mylistä¸­çš„åŸŸå
    for domain, mylist_rule in mylist.items():
        processed_domains.add(domain)
        # æ£€æŸ¥æ˜¯å¦æœ‰ä¸Šæ¸¸è§„åˆ™å†²çª
        if domain in domain_groups:
            # è®°å½•å†²çªï¼Œä½¿ç”¨mylistçš„è§„åˆ™
            mylist_conflict_rules.append(f"# äººå·¥å®¡æŸ¥åŒºï¼ˆmylistå†²çªï¼‰ï¼š{domain} - ä½¿ç”¨mylistè§„åˆ™")
            mylist_conflict_rules.append(mylist_rule)
            mylist_conflict_rules.append(f"# ä¸Šæ¸¸å†²çªè§„åˆ™ï¼š{chr(10).join(domain_groups[domain])}")
            mylist_conflict_rules.append("")  # ç©ºè¡Œåˆ†éš”
        else:
            # mylistè§„åˆ™æ— å†²çªï¼Œç›´æ¥åŠ å…¥æ™®é€šè§„åˆ™
            normal_rules.append(mylist_rule)

    # å¤„ç†émylistçš„åŸŸå
    for domain, rules in domain_groups.items():
        if domain in processed_domains:
            continue  # å·²å¤„ç†è¿‡ï¼ˆmylistä¸­çš„ï¼‰

        # DNSè§£ææ£€æŸ¥ï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰
        if domain in resolved_cache:
            resolvable = resolved_cache[domain]
        else:
            resolvable = check_domain_resolvable(domain)
            new_resolved[domain] = resolvable  # è®°å½•æ–°è§£æç»“æœ

        if not resolvable:
            continue  # ä¸å¯è§£æçš„åŸŸåè§„åˆ™åˆ é™¤

        # åˆ†ç¦»ç™½åå•å’Œé»‘åå•è§„åˆ™
        whitelist_rules = [r for r in rules if r.startswith("@@||")]
        blacklist_rules = [r for r in rules if r.startswith("||") and not r.startswith("@@||")]

        # å¤„ç†é»‘ç™½åå•å†²çª
        if whitelist_rules and blacklist_rules:
            # ä¼˜å…ˆä¿ç•™ç™½åå•ï¼Œæ”¾å…¥é»‘ç™½å†²çªå®¡æŸ¥åŒº
            black_white_conflict_rules.append(f"# äººå·¥å®¡æŸ¥åŒºï¼ˆé»‘ç™½åå•å†²çªï¼‰ï¼š{domain} - ä¿ç•™ç™½åå•")
            # é€‰æ‹©ç™½åå•ä¸­å¯èƒ½çš„ä¼˜å…ˆçº§è§„åˆ™ï¼ˆå¸¦$importantçš„ï¼‰
            selected_white = None
            for r in whitelist_rules:
                if "$important" in r:
                    selected_white = r
                    break
            if not selected_white:
                selected_white = whitelist_rules[0]
            black_white_conflict_rules.append(selected_white)
            black_white_conflict_rules.append(f"# å†²çªçš„é»‘åå•è§„åˆ™ï¼š{chr(10).join(blacklist_rules)}")
            black_white_conflict_rules.append("")  # ç©ºè¡Œåˆ†éš”
        elif whitelist_rules:
            # åªæœ‰ç™½åå•ï¼Œé€‰ä¼˜å…ˆçº§é«˜çš„
            selected = None
            for r in whitelist_rules:
                if "$important" in r:
                    selected = r
                    break
            if not selected:
                selected = whitelist_rules[0]
            normal_rules.append(selected)
        elif blacklist_rules:
            # åªæœ‰é»‘åå•ï¼Œé€‰ä¼˜å…ˆçº§é«˜çš„
            selected = None
            for r in blacklist_rules:
                if "$important" in r:
                    selected = r
                    break
            if not selected:
                selected = blacklist_rules[0]
            normal_rules.append(selected)

    # æ›´æ–°å¹¶ä¿å­˜è§£æç¼“å­˜
    resolved_cache.update(new_resolved)
    save_resolved_cache(resolved_cache)

    # å»é‡å¹¶æ’åºæ™®é€šè§„åˆ™
    normal_rules = sorted(list(set(normal_rules)))

    return mylist_conflict_rules, black_white_conflict_rules, normal_rules

def generate_final_file(mylist_conflict: list[str], black_white_conflict: list[str], normal_rules: list[str]):
    """ç”Ÿæˆæœ€ç»ˆçš„åˆå¹¶è§„åˆ™æ–‡ä»¶ï¼ŒåŒ…å«äººå·¥å®¡æŸ¥åŒºå’Œæ™®é€šè§„åˆ™åŒº"""
    current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    header = f"""# AdGuard Home åˆå¹¶è§„åˆ™æ–‡ä»¶
# è‡ªåŠ¨ç”Ÿæˆï¼šä¸‹è½½ä¸Šæ¸¸è§„åˆ™ â†’ æ ¼å¼è½¬æ¢ â†’ æ³›åŒ–åˆå¹¶ â†’ å†²çªå¤„ç† â†’ DNSéªŒè¯
# ä¸Šæ¸¸è§„åˆ™æ¥æºï¼š
{chr(10).join([f"- {url}" for url in UPSTREAM_RULES])}
# è§„åˆ™æ•°é‡ï¼š{len(normal_rules) + len(mylist_conflict) // 4 + len(black_white_conflict) // 4}  # ç”¨äºREADMEè‡ªåŠ¨æå–
# æœ€åæ›´æ–°æ—¶é—´ï¼š{current_time}  # ç²¾ç¡®åˆ°ç§’ï¼Œç”¨äºREADMEè‡ªåŠ¨æå–
# ç»´æŠ¤è€…ï¼šguandashengï¼ˆGitHub ç”¨æˆ·åï¼‰
# å®šæ—¶æ›´æ–°ï¼šæ¯ 8 å°æ—¶è‡ªåŠ¨åŒæ­¥ä¸Šæ¸¸è§„åˆ™
# ä¼˜åŒ–è¯´æ˜ï¼š
# 1. Hosts è§„åˆ™å·²è½¬æ¢ä¸º AdGuard æ ¼å¼ï¼ˆ||åŸŸå^ï¼‰
# 2. åŸºäºçº¯åŸŸåå¤„ç†ï¼Œç›¸åŒåŸŸåä¼˜å…ˆä¿ç•™ç™½åå•è§„åˆ™
# 3. ä¸mylist.txtå†²çªæ—¶ï¼Œä¿ç•™mylistè§„åˆ™å¹¶æ ‡è®°
# 4. åŸŸåéœ€é€šè¿‡223.5.5.5å’Œ8.8.8.8è§£æéªŒè¯ï¼Œå¦åˆ™ç§»é™¤
# 5. äººå·¥å®¡æŸ¥åŒºåŒ…å«å†²çªè§„åˆ™ï¼Œä¾›æ‰‹åŠ¨ç­›é€‰

"""
    # äººå·¥å®¡æŸ¥åŒºï¼ˆmylistå†²çªï¼‰
    mylist_section = []
    if mylist_conflict:
        mylist_section = [
            "\n# ========== äººå·¥å®¡æŸ¥åŒºï¼šmylistå†²çªè§„åˆ™ ==========",
            "# ä»¥ä¸‹è§„åˆ™ä¸mylist.txtä¸­çš„è§„åˆ™å†²çªï¼Œå·²ä½¿ç”¨mylistç‰ˆæœ¬",
            "# å»ºè®®æ£€æŸ¥å¹¶ç¡®è®¤æ˜¯å¦ä¿ç•™",
            ""
        ] + mylist_conflict

    # äººå·¥å®¡æŸ¥åŒºï¼ˆé»‘ç™½åå•å†²çªï¼‰
    black_white_section = []
    if black_white_conflict:
        black_white_section = [
            "\n# ========== äººå·¥å®¡æŸ¥åŒºï¼šé»‘ç™½åå•å†²çªè§„åˆ™ ==========",
            "# ä»¥ä¸‹è§„åˆ™å­˜åœ¨ç›¸åŒåŸŸåçš„é»‘ç™½åå•å†²çªï¼Œå·²ä¿ç•™ç™½åå•ç‰ˆæœ¬",
            "# å»ºè®®æ£€æŸ¥å¹¶ç¡®è®¤æ˜¯å¦ä¿ç•™",
            ""
        ] + black_white_conflict

    # æ™®é€šè§„åˆ™åŒº
    normal_section = [
        "\n# ========== æ ‡å‡†è§„åˆ™åŒº ==========",
        "# ç»è¿‡éªŒè¯çš„æœ‰æ•ˆè§„åˆ™ï¼Œæ— å†²çª",
        ""
    ] + normal_rules

    # åˆå¹¶æ‰€æœ‰éƒ¨åˆ†
    all_content = [header] + mylist_section + black_white_section + normal_section

    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        f.write("\n".join(all_content))
    
    print(f"\nğŸ‰ åˆå¹¶å®Œæˆï¼æ–‡ä»¶å·²ä¿å­˜è‡³ï¼š{OUTPUT_FILE}")
    print(f"ğŸ“Š æœ€ç»ˆè§„åˆ™æ•°é‡ï¼ˆæ™®é€šåŒºï¼‰ï¼š{len(normal_rules)}")
    print(f"ğŸ” äººå·¥å®¡æŸ¥åŒºï¼ˆmylistå†²çªï¼‰ï¼š{len(mylist_conflict) // 4} ç»„")
    print(f"ğŸ” äººå·¥å®¡æŸ¥åŒºï¼ˆé»‘ç™½å†²çªï¼‰ï¼š{len(black_white_conflict) // 4} ç»„")


def main():
    print("===== AdGuard Home è§„åˆ™æ•´åˆå·¥å…·ï¼ˆä¼˜åŒ–ç‰ˆï¼‰ =====")
    print(f"ğŸ“¥ æ­£åœ¨ä¸‹è½½ {len(UPSTREAM_RULES)} ä¸ªä¸Šæ¸¸è§„åˆ™...")
    
    all_rules = []
    for url in UPSTREAM_RULES:
        rules = download_rule(url)
        all_rules.extend(rules)
    
    print(f"\nğŸ“¦ æ€»ä¸‹è½½è§„åˆ™æ•°ï¼š{len(all_rules)}")
    print("ğŸ”§ æ­£åœ¨æ•´åˆè§„åˆ™ï¼ˆDNSéªŒè¯ + mylistå¤„ç† + å†²çªè§£å†³ï¼‰...")
    
    mylist_conflict, black_white_conflict, normal_rules = merge_rules(all_rules)
    generate_final_file(mylist_conflict, black_white_conflict, normal_rules)

if __name__ == "__main__":
    main()
