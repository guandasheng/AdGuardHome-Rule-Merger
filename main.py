import requests
import re
import json
import threading
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed  # ç§»é™¤Threadå¯¼å…¥
from config import UPSTREAM_RULES, OUTPUT_FILE, SUPPORTED_RULE_TYPES, EXCLUDED_PREFIXES, DNS_SERVERS, RESOLVED_CACHE_FILE

# çº¿ç¨‹å®‰å…¨çš„è®¡æ•°å™¨å’Œé”
progress_counter = 0
progress_lock = threading.Lock()
total_rules = 0

def load_resolved_cache() -> dict:
    """åŠ è½½å·²è§£æåŸŸåçš„ç¼“å­˜"""
    try:
        with open(RESOLVED_CACHE_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        return {}

def save_resolved_cache(cache: dict):
    """ä¿å­˜åŸŸåè§£æç¼“å­˜"""
    with open(RESOLVED_CACHE_FILE, 'w', encoding='utf-8') as f:
        json.dump(cache, f, ensure_ascii=False, indent=2)

def download_rule(url: str) -> list[str]:
    """ä¸‹è½½å•ä¸ªä¸Šæ¸¸è§„åˆ™ï¼Œè¿”å›æœ‰æ•ˆè§„åˆ™åˆ—è¡¨ï¼ˆè¿‡æ»¤æ³¨é‡Š/ç©ºè¡Œï¼‰"""
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        }
        response = requests.get(url, headers=headers, timeout=60, allow_redirects=True)
        response.raise_for_status()
        response.encoding = "utf-8"
        rules = response.text.split("\n")
        comment_prefixes = EXCLUDED_PREFIXES[:3]
        valid_rules = [
            rule.strip() for rule in rules
            if rule.strip() and not rule.strip().startswith(comment_prefixes)
        ]
        print(f"âœ… æˆåŠŸä¸‹è½½ {url} | æœ‰æ•ˆè§„åˆ™æ•°ï¼š{len(valid_rules)}")
        return valid_rules
    except requests.exceptions.ConnectionError:
        print(f"âŒ ä¸‹è½½å¤±è´¥ {url} | é”™è¯¯ï¼šç½‘ç»œè¿æ¥è¶…æ—¶/æ— æ³•è®¿é—®")
        return []
    except requests.exceptions.HTTPError as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥ {url} | é”™è¯¯ï¼šHTTP çŠ¶æ€ç  {e.response.status_code}")
        return []
    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥ {url} | é”™è¯¯ï¼š{str(e)}")
        return []

def convert_hosts_to_adguard(rule: str) -> str | None:
    """å°† Hosts è§„åˆ™è½¬æ¢ä¸º AdGuard è§„åˆ™ ||åŸŸå^"""
    hosts_pattern = r"^(0\.0\.0\.0|127\.0\.0\.1|::1)\s+([a-zA-Z0-9.-]+\.[a-zA-Z]+)"
    match = re.match(hosts_pattern, rule)
    if match:
        domain = match.group(2)
        return f"||{domain}^"
    return None

def extract_rule_parts(rule: str) -> tuple[str, str, bool, bool]:
    """è§£æè§„åˆ™ï¼šè¿”å›ï¼ˆåŸºç¡€åŸŸå/æ³›åŒ–åŸŸå, å®Œæ•´è§„åˆ™, æ˜¯å¦ç™½åå•, æ˜¯å¦å¸¦importantï¼‰"""
    whitelist_pattern = r"^@@\|\|([a-zA-Z0-9.-]+\.[a-zA-Z]+)(\^.*)?$"
    blacklist_pattern = r"^\|\|([a-zA-Z0-9.-]+\.[a-zA-Z]+)(\^.*)?$"
    
    is_whitelist = False
    has_important = False
    base_domain = ""
    generalized_domain = ""
    
    whitelist_match = re.match(whitelist_pattern, rule)
    if whitelist_match:
        is_whitelist = True
        base_domain = whitelist_match.group(1)
        params = whitelist_match.group(2) or ""
        if "$important" in params:
            has_important = True
    else:
        blacklist_match = re.match(blacklist_pattern, rule)
        if blacklist_match:
            base_domain = blacklist_match.group(1)
            params = blacklist_match.group(2) or ""
            if "$important" in params:
                has_important = True
    
    if base_domain:
        num_pattern = r"^([a-zA-Z]+)\d+\.(.*)$"
        num_match = re.match(num_pattern, base_domain)
        if num_match:
            generalized_domain = f"{num_match.group(1)}*.{num_match.group(2)}"
        else:
            generalized_domain = base_domain
    
    return (generalized_domain, rule, is_whitelist, has_important)

def merge_rules(all_rules: list[str]) -> list[str]:
    """æ•´åˆè§„åˆ™ï¼šæ³›åŒ–åˆå¹¶ã€é»‘ç™½åå•å†²çªå¤„ç†ã€ä¼˜å…ˆçº§ä¿ç•™"""
    rule_groups = defaultdict(dict)
    
    for rule in all_rules:
        converted_rule = convert_hosts_to_adguard(rule)
        final_rule = converted_rule if converted_rule else rule
        
        if not any(final_rule.startswith(prefix) for prefix in ["||", "@@||"]):
            continue
        
        generalized_domain, full_rule, is_whitelist, has_important = extract_rule_parts(final_rule)
        if not generalized_domain:
            continue
        
        domain_group = rule_groups[generalized_domain]
        
        if is_whitelist:
            if is_whitelist not in domain_group:
                domain_group[is_whitelist] = {}
            if has_important or not domain_group[is_whitelist]:
                domain_group[is_whitelist][has_important] = full_rule
        else:
            if True not in domain_group:
                if is_whitelist not in domain_group:
                    domain_group[is_whitelist] = {}
                if has_important or not domain_group[is_whitelist]:
                    domain_group[is_whitelist][has_important] = full_rule
    
    final_rules = []
    for domain, groups in rule_groups.items():
        if True in groups:
            whitelist_group = groups[True]
            if True in whitelist_group:
                final_rules.append(whitelist_group[True])
            else:
                final_rules.append(next(iter(whitelist_group.values())))
        else:
            blacklist_group = groups[False]
            if True in blacklist_group:
                final_rules.append(blacklist_group[True])
            else:
                final_rules.append(next(iter(blacklist_group.values())))
    
    final_rules.sort()
    return final_rules

def resolve_domain(domain: str, dns_servers: list[str], retries: int = 2) -> bool:
    """è§£æåŸŸåï¼Œæ”¯æŒé‡è¯•å’Œå¤šæœåŠ¡å™¨åˆ‡æ¢"""
    import dns.resolver
    resolver = dns.resolver.Resolver(configure=False)
    resolver.timeout = 5
    resolver.lifetime = 10

    for _ in range(retries):
        for server in dns_servers:
            try:
                resolver.nameservers = [server]
                answers = resolver.resolve(domain, 'A')
                return len(answers) > 0
            except dns.resolver.NXDOMAIN:
                return False
            except (dns.resolver.Timeout, dns.resolver.NoNameservers, dns.resolver.SERVFAIL):
                continue
            except Exception:
                continue
    return False

def extract_original_domain(rule: str) -> str:
    """ä»è§„åˆ™ä¸­æå–åŸå§‹åŸŸå"""
    if rule.startswith("@@||"):
        match = re.match(r"^@@\|\|([a-zA-Z0-9.-]+\.[a-zA-Z]+)", rule)
    elif rule.startswith("||"):
        match = re.match(r"^\|\|([a-zA-Z0-9.-]+\.[a-zA-Z]+)", rule)
    else:
        return ""
    return match.group(1) if match else ""

def process_rule(rule: str, resolved_cache: dict, cache_lock: threading.Lock) -> str | None:
    """å¤„ç†å•ä¸ªè§„åˆ™ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
    global progress_counter, total_rules
    
    original_domain = extract_original_domain(rule)
    if not original_domain:
        with progress_lock:
            global progress_counter
            progress_counter += 1
        return rule
    
    # æ£€æŸ¥ç¼“å­˜
    with cache_lock:
        if original_domain in resolved_cache:
            is_valid = resolved_cache[original_domain]
            with progress_lock:
                progress_counter += 1
            return rule if is_valid else None
    
    # è§£æåŸŸå
    try:
        is_valid = resolve_domain(original_domain, DNS_SERVERS)
        with cache_lock:
            resolved_cache[original_domain] = is_valid
        with progress_lock:
            progress_counter += 1
        return rule if is_valid else None
    except Exception as e:
        print(f" | è§£æé”™è¯¯ - {original_domain}: {str(e)}")
        with progress_lock:
            progress_counter += 1
        return rule  # è§£æé”™è¯¯æ—¶ä¿ç•™è§„åˆ™

def filter_unresolvable_domains(rules: list[str]) -> list[str]:
    """å¤šçº¿ç¨‹è¿‡æ»¤æ— æ³•è§£æçš„åŸŸåè§„åˆ™"""
    global total_rules, progress_counter
    total_rules = len(rules)
    progress_counter = 0
    resolved_cache = load_resolved_cache()
    cache_lock = threading.Lock()
    valid_rules = []
    
    print(f"å¼€å§‹è¿‡æ»¤æ— æ•ˆåŸŸåï¼ˆæ€»è§„åˆ™æ•°ï¼š{total_rules}ï¼Œçº¿ç¨‹æ•°ï¼š100ï¼‰...")
    
    with ThreadPoolExecutor(max_workers=100) as executor:
        futures = [executor.submit(process_rule, rule, resolved_cache, cache_lock) for rule in rules]
        
        for future in as_completed(futures):
            result = future.result()
            if result:
                valid_rules.append(result)
            
            # æ˜¾ç¤ºè¿›åº¦
            with progress_lock:
                progress = (progress_counter / total_rules) * 100
                if progress_counter % 100 == 0 or progress_counter == total_rules:
                    print(f"\r[{'#' * int(progress / 2)}{'-' * (50 - int(progress / 2))}] {progress:.1f}% | {progress_counter}/{total_rules}", end="")
    
    print()
    save_resolved_cache(resolved_cache)
    return valid_rules

def generate_final_file(rules: list[str]):
    """ç”Ÿæˆæœ€ç»ˆçš„åˆå¹¶è§„åˆ™æ–‡ä»¶"""
    from datetime import datetime
    current_time = datetime.utcnow().strftime("%Y-%m-%d %H:%M:%S UTC")
    
    header = f"""# AdGuard Home åˆå¹¶è§„åˆ™æ–‡ä»¶
# è‡ªåŠ¨ç”Ÿæˆï¼šä¸‹è½½ä¸Šæ¸¸è§„åˆ™ â†’ æ ¼å¼è½¬æ¢ â†’ æ³›åŒ–åˆå¹¶ â†’ å†²çªå¤„ç† â†’ æ— æ•ˆåŸŸåè¿‡æ»¤
# ä¸Šæ¸¸è§„åˆ™æ¥æºï¼š
{chr(10).join([f"- {url}" for url in UPSTREAM_RULES])}
# è§„åˆ™æ•°é‡ï¼š{len(rules)}  # ç”¨äºREADMEè‡ªåŠ¨æå–ï¼ˆè¯·å‹¿ä¿®æ”¹æ­¤è¡Œæ ¼å¼ï¼‰
# æœ€åæ›´æ–°æ—¶é—´ï¼š{current_time}  # ç”¨äºREADMEè‡ªåŠ¨æå–ï¼ˆè¯·å‹¿ä¿®æ”¹æ­¤è¡Œæ ¼å¼ï¼‰
# ç»´æŠ¤è€…ï¼šguandashengï¼ˆGitHub ç”¨æˆ·åï¼‰
# å®šæ—¶æ›´æ–°ï¼šæ¯ 8 å°æ—¶è‡ªåŠ¨åŒæ­¥ä¸Šæ¸¸è§„åˆ™
# ä¼˜åŒ–è¯´æ˜ï¼š
# 1. Hosts è§„åˆ™å·²è½¬æ¢ä¸º AdGuard æ ¼å¼ï¼ˆ||åŸŸå^ï¼‰
# 2. æ•°å­—åç¼€å­åŸŸåè‡ªåŠ¨æ³›åŒ–ï¼ˆå¦‚ a36243.actonservice.com â†’ a*.actonservice.comï¼‰
# 3. é»‘ç™½åå•å†²çªæ—¶ï¼Œä¼˜å…ˆä¿ç•™ç™½åå•è§„åˆ™
# 4. ç›¸åŒåŸŸåä¿ç•™å¸¦ $important ä¼˜å…ˆçº§çš„è§„åˆ™
# 5. è¿‡æ»¤æ— æ³•è§£æçš„æ— æ•ˆåŸŸåï¼ˆå¤šçº¿ç¨‹å¤„ç†ï¼‰
# 6. æ‰€æœ‰è§„åˆ™å·²å»é‡å¹¶æŒ‰åŸŸåæ’åº

"""
    
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        f.write(header)
        f.write("\n".join(rules))
    
    print(f"\nğŸ‰ åˆå¹¶å®Œæˆï¼æ–‡ä»¶å·²ä¿å­˜è‡³ï¼š{OUTPUT_FILE}")
    print(f"ğŸ“Š æœ€ç»ˆè§„åˆ™æ•°é‡ï¼š{len(rules)}")


def main():
    print("===== AdGuard Home è§„åˆ™æ•´åˆå·¥å…·ï¼ˆå¤šçº¿ç¨‹ä¼˜åŒ–ç‰ˆï¼‰ =====")
    print(f"ğŸ“¥ æ­£åœ¨ä¸‹è½½ {len(UPSTREAM_RULES)} ä¸ªä¸Šæ¸¸è§„åˆ™...")
    
    all_rules = []
    for url in UPSTREAM_RULES:
        rules = download_rule(url)
        all_rules.extend(rules)
    
    print(f"\nğŸ“¦ æ€»ä¸‹è½½è§„åˆ™æ•°ï¼š{len(all_rules)}")
    if len(all_rules) == 0:
        print("âš ï¸ è­¦å‘Šï¼šæœªè·å–åˆ°ä»»ä½•æœ‰æ•ˆè§„åˆ™ï¼Œå¯èƒ½ä¸Šæ¸¸é“¾æ¥å…¨éƒ¨å¤±æ•ˆ")
        return
    
    print("ğŸ”§ æ­£åœ¨æ•´åˆè§„åˆ™ï¼ˆæ³›åŒ–åˆå¹¶ + ä¼˜å…ˆçº§å¤„ç† + å†²çªè§£å†³ï¼‰...")
    merged_rules = merge_rules(all_rules)
    
    print("ğŸ” æ­£åœ¨è¿‡æ»¤æ— æ•ˆåŸŸåï¼ˆå¤šçº¿ç¨‹æ¨¡å¼ï¼‰...")
    valid_rules = filter_unresolvable_domains(merged_rules)
    
    generate_final_file(valid_rules)

if __name__ == "__main__":
    main()
