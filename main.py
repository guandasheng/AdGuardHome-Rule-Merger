import requests
import re
from collections import defaultdict
from config import UPSTREAM_RULES, OUTPUT_FILE, EXCLUDED_PREFIXES, MYLIST_FILE

def load_mylist_rules() -> list[str]:
    """åŠ è½½æœ¬åœ°mylistè§„åˆ™ï¼Œè¿‡æ»¤æ³¨é‡Šå’Œç©ºè¡Œ"""
    try:
        with open(MYLIST_FILE, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        # è¿‡æ»¤æ³¨é‡Šï¼ˆ#/!/\/\/å¼€å¤´ï¼‰å’Œç©ºè¡Œ
        valid_rules = [
            line.strip() for line in lines
            if line.strip() and not line.strip().startswith(('#', '!', '//'))
        ]
        print(f"âœ… æˆåŠŸåŠ è½½æœ¬åœ°è§„åˆ™ {MYLIST_FILE} | æœ‰æ•ˆè§„åˆ™æ•°ï¼š{len(valid_rules)}")
        return valid_rules
    except Exception as e:
        print(f"âŒ åŠ è½½æœ¬åœ°è§„åˆ™å¤±è´¥ {MYLIST_FILE} | é”™è¯¯ï¼š{str(e)}")
        return []

def download_rule(url: str) -> list[str]:
    """ä¸‹è½½å•ä¸ªä¸Šæ¸¸è§„åˆ™ï¼Œè¿”å›æœ‰æ•ˆè§„åˆ™åˆ—è¡¨ï¼ˆè¿‡æ»¤æ³¨é‡Š/ç©ºè¡Œï¼‰"""
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        }
        response = requests.get(url, headers=headers, timeout=60, allow_redirects=True)
        response.raise_for_status()
        response.encoding = "utf-8"
        rules = response.text.split("\n")
        comment_prefixes = EXCLUDED_PREFIXES[:3]
        valid_rules = [
            rule.strip() for rule in rules
            if rule.strip() and not rule.strip().startswith(comment_prefixes)
        ]
        print(f"âœ… æˆåŠŸä¸‹è½½ {url} | æœ‰æ•ˆè§„åˆ™æ•°ï¼š{len(valid_rules)}")
        return valid_rules
    except requests.exceptions.ConnectionError:
        print(f"âŒ ä¸‹è½½å¤±è´¥ {url} | é”™è¯¯ï¼šç½‘ç»œè¿æ¥è¶…æ—¶/æ— æ³•è®¿é—®")
        return []
    except requests.exceptions.HTTPError as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥ {url} | é”™è¯¯ï¼šHTTP çŠ¶æ€ç  {e.response.status_code}")
        return []
    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥ {url} | é”™è¯¯ï¼š{str(e)}")
        return []

def convert_hosts_to_adguard(rule: str) -> str | None:
    """å°† Hosts è§„åˆ™è½¬æ¢ä¸º AdGuard è§„åˆ™ ||åŸŸå^"""
    hosts_pattern = r"^(0\.0\.0\.0|127\.0\.0\.1|::1)\s+([a-zA-Z0-9.-]+\.[a-zA-Z]+)"
    match = re.match(hosts_pattern, rule)
    if match:
        domain = match.group(2)
        return f"||{domain}^"
    return None

def extract_rule_parts(rule: str) -> tuple[str, str, bool, bool]:
    """è§£æè§„åˆ™ï¼šè¿”å›ï¼ˆåŸºç¡€åŸŸå/æ³›åŒ–åŸŸå, å®Œæ•´è§„åˆ™, æ˜¯å¦ç™½åå•, æ˜¯å¦å¸¦importantï¼‰"""
    # åŒ¹é…ç™½åå•è§„åˆ™ @@||åŸŸå^$å‚æ•° æˆ– @@||åŸŸå^
    whitelist_pattern = r"^@@\|\|([a-zA-Z0-9.-]+\.[a-zA-Z]+)(\^.*)?$"
    # åŒ¹é…é»‘åå•è§„åˆ™ ||åŸŸå^$å‚æ•° æˆ– ||åŸŸå^
    blacklist_pattern = r"^\|\|([a-zA-Z0-9.-]+\.[a-zA-Z]+)(\^.*)?$"
    
    is_whitelist = False
    has_important = False
    base_domain = ""
    generalized_domain = ""
    
    # å¤„ç†ç™½åå•
    whitelist_match = re.match(whitelist_pattern, rule)
    if whitelist_match:
        is_whitelist = True
        base_domain = whitelist_match.group(1)
        params = whitelist_match.group(2) or ""
        if "$important" in params:
            has_important = True
    else:
        # å¤„ç†é»‘åå•
        blacklist_match = re.match(blacklist_pattern, rule)
        if blacklist_match:
            base_domain = blacklist_match.group(1)
            params = blacklist_match.group(2) or ""
            if "$important" in params:
                has_important = True
    
    if base_domain:
        # ç”Ÿæˆæ³›åŒ–åŸŸåï¼ˆå¦‚ a36243.actonservice.com â†’ a*.actonservice.comï¼‰
        num_pattern = r"^([a-zA-Z]+)\d+\.(.*)$"
        num_match = re.match(num_pattern, base_domain)
        if num_match:
            generalized_domain = f"{num_match.group(1)}*.{num_match.group(2)}"
        else:
            generalized_domain = base_domain
    
    return (generalized_domain, rule, is_whitelist, has_important)

def merge_rules(mylist_rules: list[str], upstream_rules: list[str]) -> list[str]:
    """æ•´åˆè§„åˆ™ï¼šæœ¬åœ°è§„åˆ™ä¼˜å…ˆï¼Œæ³›åŒ–åˆå¹¶ã€é»‘ç™½åå•å†²çªå¤„ç†ã€ä¼˜å…ˆçº§ä¿ç•™"""
    rule_groups = defaultdict(dict)  # key: æ³›åŒ–åŸŸå, value: {is_whitelist: {has_important: rule}}
    mylist_domains = set()  # å­˜å‚¨mylistä¸­å·²æœ‰çš„æ³›åŒ–åŸŸå
    
    # ç¬¬ä¸€æ­¥ï¼šå¤„ç†æœ¬åœ°è§„åˆ™ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
    for rule in mylist_rules:
        converted_rule = convert_hosts_to_adguard(rule)
        final_rule = converted_rule if converted_rule else rule
        
        if not any(final_rule.startswith(prefix) for prefix in ["||", "@@||"]):
            continue
        
        generalized_domain, full_rule, is_whitelist, has_important = extract_rule_parts(final_rule)
        if not generalized_domain:
            continue
        
        # æ·»åŠ åˆ°æœ¬åœ°åŸŸåé›†åˆ
        mylist_domains.add(generalized_domain)
        
        # å­˜å‚¨æœ¬åœ°è§„åˆ™
        domain_group = rule_groups[generalized_domain]
        if is_whitelist not in domain_group:
            domain_group[is_whitelist] = {}
        # æœ¬åœ°è§„åˆ™ç›´æ¥è¦†ç›–ï¼Œä¸è€ƒè™‘æ˜¯å¦æœ‰important
        domain_group[is_whitelist][has_important] = full_rule
    
    # ç¬¬äºŒæ­¥ï¼šå¤„ç†ä¸Šæ¸¸è§„åˆ™ï¼ˆä»…ä¿ç•™æœ¬åœ°è§„åˆ™ä¸­ä¸å­˜åœ¨çš„æ³›åŒ–åŸŸåï¼‰
    for rule in upstream_rules:
        converted_rule = convert_hosts_to_adguard(rule)
        final_rule = converted_rule if converted_rule else rule
        
        if not any(final_rule.startswith(prefix) for prefix in ["||", "@@||"]):
            continue
        
        generalized_domain, full_rule, is_whitelist, has_important = extract_rule_parts(final_rule)
        if not generalized_domain:
            continue
        
        # è‹¥æ³›åŒ–åŸŸåå·²å­˜åœ¨äºæœ¬åœ°è§„åˆ™ï¼Œåˆ™è·³è¿‡ä¸Šæ¸¸è§„åˆ™
        if generalized_domain in mylist_domains:
            continue
        
        # å¤„ç†ä¸Šæ¸¸è§„åˆ™
        domain_group = rule_groups[generalized_domain]
        if is_whitelist:
            if is_whitelist not in domain_group:
                domain_group[is_whitelist] = {}
            # ä¸Šæ¸¸è§„åˆ™ï¼šæœ‰importantçš„ä¼˜å…ˆï¼Œæˆ–è€…å½“å‰æ²¡æœ‰è§„åˆ™æ—¶æ·»åŠ 
            if has_important or not domain_group[is_whitelist]:
                domain_group[is_whitelist][has_important] = full_rule
        else:
            # é»‘åå•ï¼šä»…å½“æ²¡æœ‰ç™½åå•æ—¶æ‰å¤„ç†
            if True not in domain_group:
                if is_whitelist not in domain_group:
                    domain_group[is_whitelist] = {}
                if has_important or not domain_group[is_whitelist]:
                    domain_group[is_whitelist][has_important] = full_rule
    
    # ç”Ÿæˆæœ€ç»ˆè§„åˆ™åˆ—è¡¨
    final_rules = []
    for domain, groups in rule_groups.items():
        if True in groups:  # å­˜åœ¨ç™½åå•
            whitelist_group = groups[True]
            # ä¼˜å…ˆé€‰æ‹©å¸¦importantçš„è§„åˆ™
            if True in whitelist_group:
                final_rules.append(whitelist_group[True])
            else:
                final_rules.append(next(iter(whitelist_group.values())))
        else:  # ä»…é»‘åå•
            blacklist_group = groups[False]
            if True in blacklist_group:
                final_rules.append(blacklist_group[True])
            else:
                final_rules.append(next(iter(blacklist_group.values())))
    
    # æŒ‰åŸŸåæ’åº
    final_rules.sort()
    return final_rules

def generate_final_file(rules: list[str]):
    """ç”Ÿæˆæœ€ç»ˆçš„åˆå¹¶è§„åˆ™æ–‡ä»¶"""
    from datetime import datetime
    current_time = datetime.utcnow().strftime("%Y-%m-%d %H:%M:%S UTC")
    
    header = f"""# AdGuard Home åˆå¹¶è§„åˆ™æ–‡ä»¶
# è‡ªåŠ¨ç”Ÿæˆï¼šä¸‹è½½ä¸Šæ¸¸è§„åˆ™ â†’ æ ¼å¼è½¬æ¢ â†’ æ³›åŒ–åˆå¹¶ â†’ å†²çªå¤„ç†
# ä¸Šæ¸¸è§„åˆ™æ¥æºï¼š
{chr(10).join([f"- {url}" for url in UPSTREAM_RULES])}
# æœ¬åœ°è§„åˆ™æ¥æºï¼š{MYLIST_FILE}
# è§„åˆ™æ•°é‡ï¼š{len(rules)}  # ç”¨äºREADMEè‡ªåŠ¨æå–ï¼ˆè¯·å‹¿ä¿®æ”¹æ­¤è¡Œæ ¼å¼ï¼‰
# æœ€åæ›´æ–°æ—¶é—´ï¼š{current_time}  # ç”¨äºREADMEè‡ªåŠ¨æå–ï¼ˆè¯·å‹¿ä¿®æ”¹æ­¤è¡Œæ ¼å¼ï¼‰
# ç»´æŠ¤è€…ï¼šguandashengï¼ˆGitHub ç”¨æˆ·åï¼‰
# å®šæ—¶æ›´æ–°ï¼šæ¯ 8 å°æ—¶è‡ªåŠ¨åŒæ­¥ä¸Šæ¸¸è§„åˆ™
# ä¼˜åŒ–è¯´æ˜ï¼š
# 1. Hosts è§„åˆ™å·²è½¬æ¢ä¸º AdGuard æ ¼å¼ï¼ˆ||åŸŸå^ï¼‰
# 2. æ•°å­—åç¼€å­åŸŸåè‡ªåŠ¨æ³›åŒ–ï¼ˆå¦‚ a36243.actonservice.com â†’ a*.actonservice.comï¼‰
# 3. æœ¬åœ°è§„åˆ™(mylist.txt)ä¼˜å…ˆçº§æœ€é«˜ï¼Œä¼šè¦†ç›–ä¸Šæ¸¸æ‰€æœ‰ç›¸åŒåŸŸåè§„åˆ™
# 4. ç›¸åŒåŸŸåä¿ç•™å¸¦ $important ä¼˜å…ˆçº§çš„è§„åˆ™
# 5. æ‰€æœ‰è§„åˆ™å·²å»é‡å¹¶æŒ‰åŸŸåæ’åº

"""
    
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        f.write(header)
        f.write("\n".join(rules))
    
    print(f"\nğŸ‰ åˆå¹¶å®Œæˆï¼æ–‡ä»¶å·²ä¿å­˜è‡³ï¼š{OUTPUT_FILE}")
    print(f"ğŸ“Š æœ€ç»ˆè§„åˆ™æ•°é‡ï¼š{len(rules)}")


def main():
    print("===== AdGuard Home è§„åˆ™æ•´åˆå·¥å…·ï¼ˆä¼˜åŒ–ç‰ˆï¼‰ =====")
    
    # åŠ è½½æœ¬åœ°è§„åˆ™
    mylist_rules = load_mylist_rules()
    
    # ä¸‹è½½ä¸Šæ¸¸è§„åˆ™
    print(f"ğŸ“¥ æ­£åœ¨ä¸‹è½½ {len(UPSTREAM_RULES)} ä¸ªä¸Šæ¸¸è§„åˆ™...")
    all_upstream_rules = []
    for url in UPSTREAM_RULES:
        rules = download_rule(url)
        all_upstream_rules.extend(rules)
    
    print(f"\nğŸ“¦ æœ¬åœ°è§„åˆ™æ•°ï¼š{len(mylist_rules)} | ä¸Šæ¸¸æ€»è§„åˆ™æ•°ï¼š{len(all_upstream_rules)}")
    if len(mylist_rules) == 0 and len(all_upstream_rules) == 0:
        print("âš ï¸ è­¦å‘Šï¼šæœªè·å–åˆ°ä»»ä½•æœ‰æ•ˆè§„åˆ™ï¼Œå¯èƒ½ä¸Šæ¸¸é“¾æ¥å…¨éƒ¨å¤±æ•ˆ")
    
    # åˆå¹¶è§„åˆ™ï¼ˆæœ¬åœ°è§„åˆ™ä¼˜å…ˆï¼‰
    print("ğŸ”§ æ­£åœ¨æ•´åˆè§„åˆ™ï¼ˆæœ¬åœ°è§„åˆ™ä¼˜å…ˆ + æ³›åŒ–åˆå¹¶ + ä¼˜å…ˆçº§å¤„ç†ï¼‰...")
    merged_rules = merge_rules(mylist_rules, all_upstream_rules)
    generate_final_file(merged_rules)

if __name__ == "__main__":
    main()
