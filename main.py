import requests
import re
import json
import threading
import time
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed, TimeoutError
from config import UPSTREAM_RULES, OUTPUT_FILE, SUPPORTED_RULE_TYPES, EXCLUDED_PREFIXES, DNS_SERVERS, RESOLVED_CACHE_FILE, MYLIST_FILE

# çº¿ç¨‹å®‰å…¨çš„è®¡æ•°å™¨å’Œé”
progress_counter = 0
progress_lock = threading.Lock()
total_rules = 0
# æ–°å¢æ´»è·ƒçº¿ç¨‹è®¡æ•°å™¨ç”¨äºç›‘æ§
active_threads = 0
active_threads_lock = threading.Lock()

def load_resolved_cache() -> dict:
    """åŠ è½½å·²è§£æåŸŸåçš„ç¼“å­˜"""
    try:
        with open(RESOLVED_CACHE_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        return {}

def save_resolved_cache(cache: dict):
    """ä¿å­˜åŸŸåè§£æç¼“å­˜"""
    with open(RESOLVED_CACHE_FILE, 'w', encoding='utf-8') as f:
        json.dump(cache, f, ensure_ascii=False, indent=2)

def download_rule(url: str) -> list[str]:
    """ä¸‹è½½å•ä¸ªä¸Šæ¸¸è§„åˆ™ï¼Œè¿”å›æœ‰æ•ˆè§„åˆ™åˆ—è¡¨ï¼ˆè¿‡æ»¤æ³¨é‡Š/ç©ºè¡Œï¼‰"""
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        }
        response = requests.get(url, headers=headers, timeout=60, allow_redirects=True)
        response.raise_for_status()
        response.encoding = "utf-8"
        rules = response.text.split("\n")
        comment_prefixes = EXCLUDED_PREFIXES[:3]
        valid_rules = [
            rule.strip() for rule in rules
            if rule.strip() and not rule.strip().startswith(comment_prefixes)
        ]
        print(f"âœ… æˆåŠŸä¸‹è½½ {url} | æœ‰æ•ˆè§„åˆ™æ•°ï¼š{len(valid_rules)}")
        return valid_rules
    except requests.exceptions.ConnectionError:
        print(f"âŒ ä¸‹è½½å¤±è´¥ {url} | é”™è¯¯ï¼šç½‘ç»œè¿æ¥è¶…æ—¶/æ— æ³•è®¿é—®")
        return []
    except requests.exceptions.HTTPError as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥ {url} | é”™è¯¯ï¼šHTTP çŠ¶æ€ç  {e.response.status_code}")
        return []
    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥ {url} | é”™è¯¯ï¼š{str(e)}")
        return []

def convert_hosts_to_adguard(rule: str) -> str | None:
    """å°† Hosts è§„åˆ™è½¬æ¢ä¸º AdGuard è§„åˆ™ ||åŸŸå^"""
    hosts_pattern = r"^(0\.0\.0\.0|127\.0\.0\.1|::1)\s+([a-zA-Z0-9.-]+\.[a-zA-Z]+)"
    match = re.match(hosts_pattern, rule)
    if match:
        domain = match.group(2)
        return f"||{domain}^"
    return None

def extract_rule_parts(rule: str) -> tuple[str, str, bool, bool]:
    """è§£æè§„åˆ™ï¼šè¿”å›ï¼ˆåŸºç¡€åŸŸå/æ³›åŒ–åŸŸå, å®Œæ•´è§„åˆ™, æ˜¯å¦ç™½åå•, æ˜¯å¦å¸¦importantï¼‰"""
    whitelist_pattern = r"^@@\|\|([a-zA-Z0-9.-]+\.[a-zA-Z]+)(\^.*)?$"
    blacklist_pattern = r"^\|\|([a-zA-Z0-9.-]+\.[a-zA-Z]+)(\^.*)?$"
    
    is_whitelist = False
    has_important = False
    base_domain = ""
    generalized_domain = ""
    
    whitelist_match = re.match(whitelist_pattern, rule)
    if whitelist_match:
        is_whitelist = True
        base_domain = whitelist_match.group(1)
        params = whitelist_match.group(2) or ""
        if "$important" in params:
            has_important = True
    else:
        blacklist_match = re.match(blacklist_pattern, rule)
        if blacklist_match:
            base_domain = blacklist_match.group(1)
            params = blacklist_match.group(2) or ""
            if "$important" in params:
                has_important = True
    
    if base_domain:
        num_pattern = r"^([a-zA-Z]+)\d+\.(.*)$"
        num_match = re.match(num_pattern, base_domain)
        if num_match:
            generalized_domain = f"{num_match.group(1)}*.{num_match.group(2)}"
        else:
            generalized_domain = base_domain
    
    return (generalized_domain, rule, is_whitelist, has_important)

def merge_rules(all_rules: list[str]) -> list[str]:
    """æ•´åˆè§„åˆ™ï¼šæ³›åŒ–åˆå¹¶ã€é»‘ç™½åå•å†²çªå¤„ç†ã€ä¼˜å…ˆçº§ä¿ç•™"""
    rule_groups = defaultdict(dict)
    
    for rule in all_rules:
        converted_rule = convert_hosts_to_adguard(rule)
        final_rule = converted_rule if converted_rule else rule
        
        if not any(final_rule.startswith(prefix) for prefix in ["||", "@@||"]):
            continue
        
        generalized_domain, full_rule, is_whitelist, has_important = extract_rule_parts(final_rule)
        if not generalized_domain:
            continue
        
        domain_group = rule_groups[generalized_domain]
        
        if is_whitelist:
            if is_whitelist not in domain_group:
                domain_group[is_whitelist] = {}
            if has_important or not domain_group[is_whitelist]:
                domain_group[is_whitelist][has_important] = full_rule
        else:
            if True not in domain_group:
                if is_whitelist not in domain_group:
                    domain_group[is_whitelist] = {}
                if has_important or not domain_group[is_whitelist]:
                    domain_group[is_whitelist][has_important] = full_rule
    
    final_rules = []
    for domain, groups in rule_groups.items():
        if True in groups:
            whitelist_group = groups[True]
            if True in whitelist_group:
                final_rules.append(whitelist_group[True])
            else:
                final_rules.append(next(iter(whitelist_group.values())))
        else:
            blacklist_group = groups[False]
            if True in blacklist_group:
                final_rules.append(blacklist_group[True])
            else:
                final_rules.append(next(iter(blacklist_group.values())))
    
    final_rules.sort()
    return final_rules

def resolve_domain(domain: str, dns_servers: list[str], retries: int = 1) -> bool:
    """è§£æåŸŸåï¼Œä¼˜åŒ–è¶…æ—¶è®¾ç½®ï¼Œå‡å°‘é‡è¯•æ¬¡æ•°"""
    import dns.resolver
    resolver = dns.resolver.Resolver(configure=False)
    resolver.timeout = 3  # ç¼©çŸ­è¶…æ—¶æ—¶é—´
    resolver.lifetime = 5  # ç¼©çŸ­æ€»ç”Ÿå‘½å‘¨æœŸ

    for _ in range(retries):
        for server in dns_servers:
            try:
                resolver.nameservers = [server]
                answers = resolver.resolve(domain, 'A')
                return len(answers) > 0
            except dns.resolver.NXDOMAIN:
                return False
            except (dns.resolver.Timeout, dns.resolver.NoNameservers, dns.resolver.SERVFAIL):
                continue
            except Exception as e:
                # è®°å½•å…·ä½“é”™è¯¯ä½†ä¸é˜»å¡
                print(f"âš ï¸ è§£æå¼‚å¸¸ {domain}@{server}: {str(e)}")
                continue
    return False

def extract_original_domain(rule: str) -> str:
    """ä»è§„åˆ™ä¸­æå–åŸå§‹åŸŸå"""
    if rule.startswith("@@||"):
        match = re.match(r"^@@\|\|([a-zA-Z0-9.-]+\.[a-zA-Z]+)", rule)
    elif rule.startswith("||"):
        match = re.match(r"^\|\|([a-zA-Z0-9.-]+\.[a-zA-Z]+)", rule)
    else:
        return ""
    return match.group(1) if match else ""

def process_rule(rule: str, resolved_cache: dict, cache_lock: threading.Lock) -> str | None:
    """å¤„ç†å•ä¸ªè§„åˆ™ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰ï¼Œå¢åŠ çº¿ç¨‹ç›‘æ§"""
    global progress_counter
    
    # ç›‘æ§æ´»è·ƒçº¿ç¨‹æ•°
    with active_threads_lock:
        global active_threads
        active_threads += 1
    
    try:
        original_domain = extract_original_domain(rule)
        if not original_domain:
            with progress_lock:
                progress_counter += 1
            return rule
        
        # æ£€æŸ¥ç¼“å­˜
        with cache_lock:
            if original_domain in resolved_cache:
                is_valid = resolved_cache[original_domain]
                with progress_lock:
                    progress_counter += 1
                return rule if is_valid else None
        
        # è§£æåŸŸåï¼ˆæ–°å¢è¶…æ—¶ä¿æŠ¤ï¼‰
        start_time = time.time()
        is_valid = resolve_domain(original_domain, DNS_SERVERS)
        elapsed = time.time() - start_time
        
        # è®°å½•æ…¢æŸ¥è¯¢
        if elapsed > 3:
            print(f"â±ï¸ æ…¢è§£æ {original_domain} è€—æ—¶ {elapsed:.2f}s")
        
        with cache_lock:
            resolved_cache[original_domain] = is_valid
        with progress_lock:
            progress_counter += 1
        return rule if is_valid else None
    
    except Exception as e:
        print(f"âŒ å¤„ç†é”™è¯¯ {original_domain}: {str(e)}")
        with progress_lock:
            progress_counter += 1
        return rule  # è§£æé”™è¯¯æ—¶ä¿ç•™è§„åˆ™
    
    finally:
        # å‡å°‘æ´»è·ƒçº¿ç¨‹è®¡æ•°
        with active_threads_lock:
            active_threads -= 1

def filter_unresolvable_domains(rules: list[str]) -> list[str]:
    """å¤šçº¿ç¨‹è¿‡æ»¤æ— æ³•è§£æçš„åŸŸåè§„åˆ™ï¼Œå¢åŠ è¶…æ—¶æ§åˆ¶å’Œè¿›åº¦ç›‘æ§"""
    global total_rules, progress_counter, active_threads
    total_rules = len(rules)
    progress_counter = 0
    active_threads = 0
    resolved_cache = load_resolved_cache()
    cache_lock = threading.Lock()
    valid_rules = []
    
    # è®¡ç®—éœ€è¦æ£€æµ‹çš„æ–°åŸŸåæ•°é‡
    new_domains_count = sum(1 for rule in rules 
                          if extract_original_domain(rule) not in resolved_cache)
    print(f"å¼€å§‹è¿‡æ»¤æ— æ•ˆåŸŸåï¼ˆæ€»è§„åˆ™æ•°ï¼š{total_rules}ï¼Œéœ€æ£€æµ‹æ–°åŸŸåï¼š{new_domains_count}ï¼Œçº¿ç¨‹æ•°ï¼š50ï¼‰...")
    
    # å‡å°‘çº¿ç¨‹æ± å¤§å°ï¼Œé¿å…èµ„æºç«äº‰
    with ThreadPoolExecutor(max_workers=50) as executor:
        futures = {executor.submit(process_rule, rule, resolved_cache, cache_lock): rule for rule in rules}
        completed = 0
        
        # å¢åŠ æ•´ä½“è¶…æ—¶æ§åˆ¶
        while completed < len(futures):
            # æ¯æ¬¡ç­‰å¾…10ç§’ï¼Œè¶…æ—¶çš„ä»»åŠ¡å¼ºåˆ¶å–æ¶ˆ
            done, not_done = concurrent.futures.wait(futures, timeout=10)
            
            for future in done:
                try:
                    result = future.result()
                    if result:
                        valid_rules.append(result)
                    completed += 1
                except Exception as e:
                    print(f"âš ï¸ ä»»åŠ¡å¼‚å¸¸: {str(e)}")
                    completed += 1
            
            # å¤„ç†è¶…æ—¶æœªå®Œæˆçš„ä»»åŠ¡
            for future in not_done:
                rule = futures[future]
                domain = extract_original_domain(rule)
                print(f"â° ä»»åŠ¡è¶…æ—¶ {domain}ï¼Œå¼ºåˆ¶ç»§ç»­")
                # è¶…æ—¶ä»»åŠ¡ç»“æœè§†ä¸ºæœ‰æ•ˆï¼ˆé¿å…è¯¯åˆ ï¼‰
                valid_rules.append(rule)
                future.cancel()
                completed += 1
                with progress_lock:
                    progress_counter += 1
            
            # å®æ—¶æ˜¾ç¤ºè¿›åº¦å’Œçº¿ç¨‹çŠ¶æ€
            with progress_lock, active_threads_lock:
                progress = (progress_counter / total_rules) * 100
                print(f"\r[{'#' * int(progress / 2)}{'-' * (50 - int(progress / 2))}] {progress:.1f}% | {progress_counter}/{total_rules} | æ´»è·ƒçº¿ç¨‹: {active_threads}", end="")
    
    print()
    save_resolved_cache(resolved_cache)
    return valid_rules

def load_mylist_rules() -> list[str]:
    """åŠ è½½æœ¬åœ°äººå·¥å®¡æŸ¥è§„åˆ™"""
    try:
        with open(MYLIST_FILE, 'r', encoding='utf-8') as f:
            rules = f.readlines()
        # è¿‡æ»¤æ³¨é‡Šå’Œç©ºè¡Œä½†ä¿ç•™æœ‰æ•ˆè§„åˆ™
        valid_rules = [
            rule.strip() for rule in rules
            if rule.strip() and not rule.strip().startswith(('#', '!', '//'))
        ]
        print(f"âœ… åŠ è½½æœ¬åœ°è§„åˆ™ {MYLIST_FILE} | æœ‰æ•ˆè§„åˆ™æ•°ï¼š{len(valid_rules)}")
        return valid_rules
    except FileNotFoundError:
        print(f"âš ï¸ æœªæ‰¾åˆ°æœ¬åœ°è§„åˆ™æ–‡ä»¶ {MYLIST_FILE}ï¼Œè·³è¿‡åŠ è½½")
        return []
    except Exception as e:
        print(f"âŒ åŠ è½½æœ¬åœ°è§„åˆ™å¤±è´¥ï¼š{str(e)}")
        return []

def generate_final_file(rules: list[str]):
    """ç”Ÿæˆæœ€ç»ˆçš„åˆå¹¶è§„åˆ™æ–‡ä»¶ï¼ŒåŒ…å«æœ¬åœ°è§„åˆ™"""
    from datetime import datetime
    current_time = datetime.utcnow().strftime("%Y-%m-%d %H:%M:%S UTC")
    
    # åŠ è½½å¹¶åˆå¹¶æœ¬åœ°è§„åˆ™
    mylist_rules = load_mylist_rules()
    final_rules = rules + mylist_rules
    # å»é‡å¤„ç†
    final_rules = list(sorted(set(final_rules)))
    
    header = f"""# AdGuard Home åˆå¹¶è§„åˆ™æ–‡ä»¶
# è‡ªåŠ¨ç”Ÿæˆï¼šä¸‹è½½ä¸Šæ¸¸è§„åˆ™ â†’ æ ¼å¼è½¬æ¢ â†’ æ³›åŒ–åˆå¹¶ â†’ å†²çªå¤„ç† â†’ æ— æ•ˆåŸŸåè¿‡æ»¤
# ä¸Šæ¸¸è§„åˆ™æ¥æºï¼š
{chr(10).join([f"- {url}" for url in UPSTREAM_RULES])}
# è§„åˆ™æ•°é‡ï¼š{len(final_rules)}  # ç”¨äºREADMEè‡ªåŠ¨æå–ï¼ˆè¯·å‹¿ä¿®æ”¹æ­¤è¡Œæ ¼å¼ï¼‰
# æœ€åæ›´æ–°æ—¶é—´ï¼š{current_time}  # ç”¨äºREADMEè‡ªåŠ¨æå–ï¼ˆè¯·å‹¿ä¿®æ”¹æ­¤è¡Œæ ¼å¼ï¼‰
# ç»´æŠ¤è€…ï¼šguandashengï¼ˆGitHub ç”¨æˆ·åï¼‰
# å®šæ—¶æ›´æ–°ï¼šæ¯ 8 å°æ—¶è‡ªåŠ¨åŒæ­¥ä¸Šæ¸¸è§„åˆ™
# ä¼˜åŒ–è¯´æ˜ï¼š
# 1. Hosts è§„åˆ™å·²è½¬æ¢ä¸º AdGuard æ ¼å¼ï¼ˆ||åŸŸå^ï¼‰
# 2. æ•°å­—åç¼€å­åŸŸåè‡ªåŠ¨æ³›åŒ–ï¼ˆå¦‚ a36243.actonservice.com â†’ a*.actonservice.comï¼‰
# 3. é»‘ç™½åå•å†²çªæ—¶ï¼Œä¼˜å…ˆä¿ç•™ç™½åå•è§„åˆ™
# 4. ç›¸åŒåŸŸåä¿ç•™å¸¦ $important ä¼˜å…ˆçº§çš„è§„åˆ™
# 5. è‡ªåŠ¨è¿‡æ»¤æ— æ•ˆåŸŸåï¼Œä¼˜åŒ–äº†è§£æè¶…æ—¶é—®é¢˜
"""
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        f.write(header + '\n'.join(final_rules) + '\n')
    print(f"âœ… è§„åˆ™æ–‡ä»¶ç”Ÿæˆå®Œæˆï¼Œä¿å­˜è‡³ {OUTPUT_FILE}ï¼ˆå…± {len(final_rules)} æ¡è§„åˆ™ï¼‰")

def main():
    """ä¸»å‡½æ•°ï¼šæ‰§è¡Œè§„åˆ™ä¸‹è½½ã€åˆå¹¶ã€è¿‡æ»¤ã€ç”Ÿæˆå®Œæ•´æµç¨‹"""
    print("====== å¼€å§‹æ‰§è¡Œ AdGuard Home è§„åˆ™åˆå¹¶ ======")
    
    # 1. ä¸‹è½½æ‰€æœ‰ä¸Šæ¸¸è§„åˆ™
    all_rules = []
    for url in UPSTREAM_RULES:
        rules = download_rule(url)
        all_rules.extend(rules)
    print(f"ğŸ“Š ä¸‹è½½å®Œæˆï¼ŒåŸå§‹è§„åˆ™æ€»æ•°ï¼š{len(all_rules)}")
    
    # 2. åˆå¹¶å»é‡è§„åˆ™
    merged_rules = merge_rules(all_rules)
    print(f"ğŸ”— è§„åˆ™åˆå¹¶å®Œæˆï¼Œå»é‡åæ€»æ•°ï¼š{len(merged_rules)}")
    
    # 3. è¿‡æ»¤æ— æ•ˆåŸŸåï¼ˆæ ¸å¿ƒä¼˜åŒ–éƒ¨åˆ†ï¼‰
    valid_rules = filter_unresolvable_domains(merged_rules)
    print(f"ğŸ¯ æ— æ•ˆåŸŸåè¿‡æ»¤å®Œæˆï¼Œæœ‰æ•ˆè§„åˆ™æ•°ï¼š{len(valid_rules)}")
    
    # 4. ç”Ÿæˆæœ€ç»ˆè§„åˆ™æ–‡ä»¶
    generate_final_file(valid_rules)
    print("====== è§„åˆ™åˆå¹¶æµç¨‹æ‰§è¡Œå®Œæ¯• ======")

if __name__ == "__main__":
    import concurrent.futures  # ç¡®ä¿å¯¼å…¥ç”¨äºè¶…æ—¶æ§åˆ¶
    main()
