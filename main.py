import requests
import re
import json
import time
from collections import defaultdict
from config import UPSTREAM_RULES, OUTPUT_FILE, SUPPORTED_RULE_TYPES, EXCLUDED_PREFIXES, DNS_SERVERS, RESOLVED_CACHE_FILE
import dns.resolver  # éœ€è¦æ–°å¢è¿™ä¸ªä¾èµ–

def print_progress(current, total, status=""):
    """æ˜¾ç¤ºè¿›åº¦æ¡"""
    percent = current / total * 100 if total > 0 else 100
    bar_length = 40
    filled_length = int(bar_length * current // total) if total > 0 else bar_length
    bar = '=' * filled_length + '-' * (bar_length - filled_length)
    print(f'\r[{bar}] {percent:.1f}% | {current}/{total} | {status}', end='', flush=True)
    if current == total:
        print()  # è¿›åº¦å®Œæˆåæ¢è¡Œ

def load_resolved_cache():
    """åŠ è½½å·²è§£æçš„åŸŸåç¼“å­˜"""
    try:
        with open(RESOLVED_CACHE_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        return {}

def save_resolved_cache(cache):
    """ä¿å­˜åŸŸåè§£æç¼“å­˜"""
    with open(RESOLVED_CACHE_FILE, 'w', encoding='utf-8') as f:
        json.dump(cache, f, ensure_ascii=False, indent=2)

def resolve_domain(domain, dns_servers):
    """è§£æåŸŸåï¼Œè¿”å›æ˜¯å¦æœ‰æ•ˆ"""
    for server in dns_servers:
        resolver = dns.resolver.Resolver()
        resolver.nameservers = [server]
        resolver.timeout = 5
        resolver.lifetime = 5
        
        try:
            answers = resolver.resolve(domain, 'A')
            for answer in answers:
                ip = str(answer)
                if ip != '0.0.0.0':
                    return True
        except (dns.resolver.NXDOMAIN, dns.resolver.Timeout, dns.resolver.NoAnswer):
            continue
    return False

def download_rule(url: str) -> list[str]:
    """ä¸‹è½½å•ä¸ªä¸Šæ¸¸è§„åˆ™ï¼Œè¿”å›æœ‰æ•ˆè§„åˆ™åˆ—è¡¨ï¼ˆè¿‡æ»¤æ³¨é‡Š/ç©ºè¡Œï¼‰"""
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        }
        response = requests.get(url, headers=headers, timeout=60, allow_redirects=True)
        response.raise_for_status()
        response.encoding = "utf-8"
        rules = response.text.split("\n")
        comment_prefixes = EXCLUDED_PREFIXES[:3]
        valid_rules = [
            rule.strip() for rule in rules
            if rule.strip() and not rule.strip().startswith(comment_prefixes)
        ]
        print(f"âœ… æˆåŠŸä¸‹è½½ {url} | æœ‰æ•ˆè§„åˆ™æ•°ï¼š{len(valid_rules)}")
        return valid_rules
    except requests.exceptions.ConnectionError:
        print(f"âŒ ä¸‹è½½å¤±è´¥ {url} | é”™è¯¯ï¼šç½‘ç»œè¿æ¥è¶…æ—¶/æ— æ³•è®¿é—®")
        return []
    except requests.exceptions.HTTPError as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥ {url} | é”™è¯¯ï¼šHTTP çŠ¶æ€ç  {e.response.status_code}")
        return []
    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥ {url} | é”™è¯¯ï¼š{str(e)}")
        return []

def convert_hosts_to_adguard(rule: str) -> str | None:
    """å°† Hosts è§„åˆ™è½¬æ¢ä¸º AdGuard è§„åˆ™ ||åŸŸå^"""
    hosts_pattern = r"^(0\.0\.0\.0|127\.0\.0\.1|::1)\s+([a-zA-Z0-9.-]+\.[a-zA-Z]+)"
    match = re.match(hosts_pattern, rule)
    if match:
        domain = match.group(2)
        return f"||{domain}^"
    return None

def extract_rule_parts(rule: str) -> tuple[str, str, bool, bool, str]:
    """è§£æè§„åˆ™ï¼šè¿”å›ï¼ˆåŸºç¡€åŸŸå/æ³›åŒ–åŸŸå, å®Œæ•´è§„åˆ™, æ˜¯å¦ç™½åå•, æ˜¯å¦å¸¦important, åŸå§‹åŸŸåï¼‰"""
    # åŒ¹é…ç™½åå•è§„åˆ™ @@||åŸŸå^$å‚æ•° æˆ– @@||åŸŸå^
    whitelist_pattern = r"^@@\|\|([a-zA-Z0-9.-]+\.[a-zA-Z]+)(\^.*)?$"
    # åŒ¹é…é»‘åå•è§„åˆ™ ||åŸŸå^$å‚æ•° æˆ– ||åŸŸå^
    blacklist_pattern = r"^\|\|([a-zA-Z0-9.-]+\.[a-zA-Z]+)(\^.*)?$"
    
    is_whitelist = False
    has_important = False
    base_domain = ""
    generalized_domain = ""
    original_domain = ""
    
    # å¤„ç†ç™½åå•
    whitelist_match = re.match(whitelist_pattern, rule)
    if whitelist_match:
        is_whitelist = True
        original_domain = whitelist_match.group(1)
        base_domain = original_domain
        params = whitelist_match.group(2) or ""
        if "$important" in params:
            has_important = True
    else:
        # å¤„ç†é»‘åå•
        blacklist_match = re.match(blacklist_pattern, rule)
        if blacklist_match:
            original_domain = blacklist_match.group(1)
            base_domain = original_domain
            params = blacklist_match.group(2) or ""
            if "$important" in params:
                has_important = True
    
    if base_domain:
        # ç”Ÿæˆæ³›åŒ–åŸŸåï¼ˆå¦‚ a36243.actonservice.com â†’ a*.actonservice.comï¼‰
        num_pattern = r"^([a-zA-Z]+)\d+\.(.*)$"
        num_match = re.match(num_pattern, base_domain)
        if num_match:
            generalized_domain = f"{num_match.group(1)}*.{num_match.group(2)}"
        else:
            generalized_domain = base_domain
    
    return (generalized_domain, rule, is_whitelist, has_important, original_domain)

def merge_rules(all_rules: list[str]) -> list[str]:
    """æ•´åˆè§„åˆ™ï¼šæ³›åŒ–åˆå¹¶ã€é»‘ç™½åå•å†²çªå¤„ç†ã€ä¼˜å…ˆçº§ä¿ç•™"""
    rule_groups = defaultdict(dict)  # key: æ³›åŒ–åŸŸå, value: {is_whitelist: {has_important: rule}}
    
    for i, rule in enumerate(all_rules):
        # æ˜¾ç¤ºå¤„ç†è¿›åº¦
        print_progress(i + 1, len(all_rules), f"å¤„ç†è§„åˆ™: {rule[:50]}...")
        
        # è½¬æ¢ Hosts è§„åˆ™
        converted_rule = convert_hosts_to_adguard(rule)
        final_rule = converted_rule if converted_rule else rule
        
        # è¿‡æ»¤ä¸æ”¯æŒçš„è§„åˆ™
        if not any(final_rule.startswith(prefix) for prefix in ["||", "@@||"]):
            continue
        
        # è§£æè§„åˆ™éƒ¨åˆ†
        generalized_domain, full_rule, is_whitelist, has_important, _ = extract_rule_parts(final_rule)
        if not generalized_domain:
            continue
        
        # æŒ‰æ³›åŒ–åŸŸååˆ†ç»„å¤„ç†
        domain_group = rule_groups[generalized_domain]
        
        # ç™½åå•ä¼˜å…ˆçº§ > é»‘åå•
        if is_whitelist:
            if is_whitelist not in domain_group:
                domain_group[is_whitelist] = {}
            if has_important or not domain_group[is_whitelist]:
                domain_group[is_whitelist][has_important] = full_rule
        else:
            # é»‘åå•ï¼šä»…å½“æ²¡æœ‰ç™½åå•æ—¶æ‰å¤„ç†
            if True not in domain_group:  # æ— ç™½åå•
                if is_whitelist not in domain_group:
                    domain_group[is_whitelist] = {}
                if has_important or not domain_group[is_whitelist]:
                    domain_group[is_whitelist][has_important] = full_rule
    
    # ç”Ÿæˆæœ€ç»ˆè§„åˆ™åˆ—è¡¨
    final_rules = []
    for domain, groups in rule_groups.items():
        if True in groups:  # å­˜åœ¨ç™½åå•
            whitelist_group = groups[True]
            if True in whitelist_group:
                final_rules.append(whitelist_group[True])
            else:
                final_rules.append(next(iter(whitelist_group.values())))
        else:  # ä»…é»‘åå•
            blacklist_group = groups[False]
            if True in blacklist_group:
                final_rules.append(blacklist_group[True])
            else:
                final_rules.append(next(iter(blacklist_group.values())))
    
    # æŒ‰åŸŸåæ’åº
    final_rules.sort()
    return final_rules

def filter_unresolvable_domains(rules):
    """è¿‡æ»¤æ— æ³•è§£æçš„åŸŸåè§„åˆ™"""
    resolved_cache = load_resolved_cache()
    valid_rules = []
    new_entries = 0
    
    print(f"\nğŸ” å¼€å§‹éªŒè¯åŸŸåè§£æçŠ¶æ€ï¼ˆä½¿ç”¨DNSæœåŠ¡å™¨: {', '.join(DNS_SERVERS)}ï¼‰")
    time.sleep(0.1)  # ç¡®ä¿è¾“å‡ºèƒ½è¢«æ­£ç¡®æ•è·
    
    for i, rule in enumerate(rules):
        # æå–åŸå§‹åŸŸå
        _, _, _, _, original_domain = extract_rule_parts(rule)
        if not original_domain:
            valid_rules.append(rule)
            print_progress(i + 1, len(rules), f"è·³è¿‡æ— æ•ˆæ ¼å¼è§„åˆ™")
            continue
        
        # æ£€æŸ¥ç¼“å­˜
        if original_domain in resolved_cache:
            if resolved_cache[original_domain]:
                valid_rules.append(rule)
            print_progress(i + 1, len(rules), f"å·²ç¼“å­˜ - {original_domain}")
            continue
        
        # éœ€è¦è§£æçš„æ–°åŸŸå
        new_entries += 1
        is_valid = resolve_domain(original_domain, DNS_SERVERS)
        resolved_cache[original_domain] = is_valid
        
        if is_valid:
            valid_rules.append(rule)
            status = f"æœ‰æ•ˆ - {original_domain}"
        else:
            status = f"æ— æ•ˆ - {original_domain}"
        
        print_progress(i + 1, len(rules), status)
        time.sleep(0.01)  # ç¨å¾®å»¶è¿Ÿï¼Œé¿å…è¾“å‡ºè¿‡å¿«
    
    # ä¿å­˜æ›´æ–°åçš„ç¼“å­˜
    save_resolved_cache(resolved_cache)
    print(f"\nğŸ“Š åŸŸåéªŒè¯å®Œæˆï¼šæ€»è§„åˆ™ {len(rules)}ï¼Œæœ‰æ•ˆè§„åˆ™ {len(valid_rules)}ï¼Œæ–°å¢ç¼“å­˜ {new_entries}")
    return valid_rules

def generate_final_file(rules: list[str]):
    """ç”Ÿæˆæœ€ç»ˆçš„åˆå¹¶è§„åˆ™æ–‡ä»¶"""
    from datetime import datetime
    current_time = datetime.utcnow().strftime("%Y-%m-%d %H:%M:%S UTC")
    
    header = f"""# AdGuard Home åˆå¹¶è§„åˆ™æ–‡ä»¶
# è‡ªåŠ¨ç”Ÿæˆï¼šä¸‹è½½ä¸Šæ¸¸è§„åˆ™ â†’ æ ¼å¼è½¬æ¢ â†’ æ³›åŒ–åˆå¹¶ â†’ å†²çªå¤„ç† â†’ åŸŸåè§£æéªŒè¯
# ä¸Šæ¸¸è§„åˆ™æ¥æºï¼š
{chr(10).join([f"- {url}" for url in UPSTREAM_RULES])}
# è§„åˆ™æ•°é‡ï¼š{len(rules)}  # ç”¨äºREADMEè‡ªåŠ¨æå–ï¼ˆè¯·å‹¿ä¿®æ”¹æ­¤è¡Œæ ¼å¼ï¼‰
# æœ€åæ›´æ–°æ—¶é—´ï¼š{current_time}  # ç”¨äºREADMEè‡ªåŠ¨æå–ï¼ˆè¯·å‹¿ä¿®æ”¹æ­¤è¡Œæ ¼å¼ï¼‰
# ç»´æŠ¤è€…ï¼šguandashengï¼ˆGitHub ç”¨æˆ·åï¼‰
# å®šæ—¶æ›´æ–°ï¼šæ¯ 8 å°æ—¶è‡ªåŠ¨åŒæ­¥ä¸Šæ¸¸è§„åˆ™
# ä¼˜åŒ–è¯´æ˜ï¼š
# 1. Hosts è§„åˆ™å·²è½¬æ¢ä¸º AdGuard æ ¼å¼ï¼ˆ||åŸŸå^ï¼‰
# 2. æ•°å­—åç¼€å­åŸŸåè‡ªåŠ¨æ³›åŒ–ï¼ˆå¦‚ a36243.actonservice.com â†’ a*.actonservice.comï¼‰
# 3. é»‘ç™½åå•å†²çªæ—¶ï¼Œä¼˜å…ˆä¿ç•™ç™½åå•è§„åˆ™
# 4. ç›¸åŒåŸŸåä¿ç•™å¸¦ $important ä¼˜å…ˆçº§çš„è§„åˆ™
# 5. æ‰€æœ‰è§„åˆ™å·²å»é‡å¹¶æŒ‰åŸŸåæ’åº
# 6. å·²è¿‡æ»¤æ— æ³•è§£ææˆ–è§£æä¸º0.0.0.0çš„åŸŸå

"""
    
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        f.write(header)
        f.write("\n".join(rules))
    
    print(f"\nğŸ‰ åˆå¹¶å®Œæˆï¼æ–‡ä»¶å·²ä¿å­˜è‡³ï¼š{OUTPUT_FILE}")
    print(f"ğŸ“Š æœ€ç»ˆè§„åˆ™æ•°é‡ï¼š{len(rules)}")


def main():
    print("===== AdGuard Home è§„åˆ™æ•´åˆå·¥å…·ï¼ˆä¼˜åŒ–ç‰ˆï¼‰ =====")
    print(f"ğŸ“¥ æ­£åœ¨ä¸‹è½½ {len(UPSTREAM_RULES)} ä¸ªä¸Šæ¸¸è§„åˆ™...")
    
    all_rules = []
    for i, url in enumerate(UPSTREAM_RULES):
        print(f"[{i+1}/{len(UPSTREAM_RULES)}] ä¸‹è½½ä¸­: {url}")
        rules = download_rule(url)
        all_rules.extend(rules)
        print_progress(i + 1, len(UPSTREAM_RULES), f"å·²ä¸‹è½½ {len(all_rules)} æ¡è§„åˆ™")
    
    print(f"\nğŸ“¦ æ€»ä¸‹è½½è§„åˆ™æ•°ï¼š{len(all_rules)}")
    if len(all_rules) == 0:
        print("âš ï¸ è­¦å‘Šï¼šæœªè·å–åˆ°ä»»ä½•æœ‰æ•ˆè§„åˆ™ï¼Œå¯èƒ½ä¸Šæ¸¸é“¾æ¥å…¨éƒ¨å¤±æ•ˆ")
        return
    
    print("\nğŸ”§ æ­£åœ¨æ•´åˆè§„åˆ™ï¼ˆæ³›åŒ–åˆå¹¶ + ä¼˜å…ˆçº§å¤„ç† + å†²çªè§£å†³ï¼‰...")
    merged_rules = merge_rules(all_rules)
    print(f"ğŸ”§ è§„åˆ™æ•´åˆå®Œæˆï¼Œåˆå¹¶åè§„åˆ™æ•°ï¼š{len(merged_rules)}")
    
    # è¿‡æ»¤æ— æ³•è§£æçš„åŸŸå
    valid_rules = filter_unresolvable_domains(merged_rules)
    
    generate_final_file(valid_rules)

if __name__ == "__main__":
    main()
